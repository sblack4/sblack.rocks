<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Debugging a OAC-BDC Connection</title><link rel=apple-touch-icon sizes=180x180 href=https://sblack.rocks/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://sblack.rocks/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://sblack.rocks/favicon-16x16.png><link rel=manifest href=https://sblack.rocks/site.webmanifest><link rel=mask-icon href=https://sblack.rocks/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><style>html body{font-family:raleway,sans-serif;background-color:#fff}:root{--accent: red;--border-width:  5px }</style><link rel=stylesheet href=https://sblack.rocks/css/main.css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Raleway"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css integrity=sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u crossorigin=anonymous><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css integrity=sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js></script><script>hljs.initHighlightingOnLoad();</script><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js></script><script src=https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js integrity=sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa crossorigin=anonymous></script><script>$(document).on('click',function(){$('.collapse').collapse('hide');})</script><meta name=generator content="Hugo 0.74.2"><script async src="https://www.googletagmanager.com/gtag/js?id=UA-116829409-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)};gtag('js',new Date());gtag('config','UA-116829409-1');</script></head><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><body><nav class="navbar navbar-default navbar-fixed-top"><div class=container><div class=navbar-header><a class="navbar-brand visible-xs" href=#>Debugging a OAC-BDC Connection</a>
<button class=navbar-toggle data-target=.navbar-collapse data-toggle=collapse>
<span class=icon-bar></span><span class=icon-bar></span><span class=icon-bar></span></button></div><div class="collapse navbar-collapse"><ul class="nav navbar-nav"><li><a href=https://sblack.rocks/>home</a></li><li><a href=https://sblack.rocks/changelog>changelog</a></li></ul><ul class="nav navbar-nav navbar-right"><li class=navbar-icon><a href=mailto:sblack.rocks@gmail.com target=_blank><i class="fa fa-envelope-o"></i></a></li><li class=navbar-icon><a href=https://github.com/sblack4 target=_blank><i class="fa fa-github"></i></a></li><li class=navbar-icon><a href=https://stackoverflow.com/users/5568528/steven-black target=_blank><i class="fa fa-stack-overflow"></i></a></li><li class=navbar-icon><a href=https://twitter.com/Genseb7 target=_blank><i class="fa fa-twitter"></i></a></li><li class=navbar-icon><a href=https://www.linkedin.com/in/steven-black/ target=_blank><i class="fa fa-linkedin"></i></a></li><li class=navbar-icon><a href=https://medium.com/@sblack4 target=_blank><i class="fa fa-medium"></i></a></li><li class=navbar-icon><a href=https://sblack.rocks/blog/index.xml target=_blank><i class="fa fa-rss-square"></i></a></li></ul></div></div></nav><main><div class=item><h4><a href=https://sblack.rocks/changelog/debugging-oac/>Debugging a OAC-BDC Connection</a></h4><h5>Debuggn a connection between Oracle Analytics Cloud & Big Data Cloud</h5><a href=https://sblack.rockstags/blog><kbd class=item-tag>blog</kbd></a>
<a href=https://sblack.rockstags/oac><kbd class=item-tag>oac</kbd></a>
<a href=https://sblack.rockstags/bdc><kbd class=item-tag>bdc</kbd></a>
<a href=https://sblack.rockstags/spark><kbd class=item-tag>spark</kbd></a></div><br><div class=text-justify><h2 id=debugging-oracle-analytics-cloud-amp-big-data-cloud>Debugging Oracle Analytics Cloud & Big Data Cloud</h2><p>My job is pretty varied but one thing that stays constant is requests to connect OAC to BDC.</p><h3 id=tail-osa-logs>Tail OSA logs</h3><p>Not everyone knows where to find these logs but just run the below command!</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>tail -f /u01/data/domain/fmw/user_projects/domains/bi/servers/AdminServer/logs/bi.log
</code></pre></div><p>In our case it reveals some cryptic output&mldr; but it seems that there is a network timeout. That means the issue probably isn&rsquo;t on the OAC side</p><pre><code class=language-log data-lang=log>####&lt;May 16, 2018 3:28:13,804 PM UTC&gt; &lt;Error&gt; &lt;oracle.bi.tech.services.dataset.StandaloneDatasetService&gt; &lt;aeoacs-bi-1&gt; &lt;bi_server1&gt; &lt;[STANDBY] ExecuteThread: '40' for queue: 'weblogic.kernel.Default (self-tuning)'&gt; &lt;&lt;anonymous&gt;&gt; &lt;&gt; &lt;0bda8caa-ead8-4bb9-ac04-2173ae17e26c-000faa87&gt; &lt;1526484493804&gt; &lt;[severity-value: 8] [rid: 0] [partition-id: 0] [partition-name: DOMAIN] &gt; &lt;BEA-000000&gt; &lt;Failed to create and convert dataset: javax.ws.rs.ProcessingException: java.net.SocketTimeoutException: Read time out after 300000 millis
</code></pre><h3 id=tail-bdc-logs>Tail BDC Logs</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>tail -f /data/var/log/spark2-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-aebigdata-bdcsce-1.out
</code></pre></div><p>reveals more cryptic errors&mldr; it looks like maybe the JVM ran out of space during a GC?</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>18/05/16 15:27:42 ERROR ResourceLeakDetector: LEAK: ByteBuf.release<span style=color:#f92672>()</span> was not called before it<span style=color:#e6db74>&#39;s garbage-collected. Enable advanced leak reporting to find out where the leak occurred. To enable advanced leak reporting, specify the JVM option &#39;</span>-Dio.netty.leakDetection.level<span style=color:#f92672>=</span>advanced<span style=color:#960050;background-color:#1e0010>&#39;</span> or call ResourceLeakDetector.setLevel<span style=color:#f92672>()</span> See http://netty.io/wiki/reference-counted-objects.html <span style=color:#66d9ef>for</span> more information.
18/05/16 15:27:43 ERROR Utils: Uncaught exception in thread task-result-getter-0
java.lang.OutOfMemoryError: Java heap space
Exception in thread <span style=color:#e6db74>&#34;task-result-getter-0&#34;</span> java.lang.OutOfMemoryError: Java heap space
18/05/16 15:37:37 ERROR LiveListenerBus: Listener EventLoggingListener threw an exception
java.io.IOException: All datanodes DatanodeInfoWithStorage<span style=color:#f92672>[</span>100.65.22.122:50010,DS-f8d560be-7098-4529-9b01-7e26b089b17e,DISK<span style=color:#f92672>]</span> are bad. Aborting...
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery<span style=color:#f92672>(</span>DFSOutputStream.java:1142<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError<span style=color:#f92672>(</span>DFSOutputStream.java:904<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run<span style=color:#f92672>(</span>DFSOutputStream.java:411<span style=color:#f92672>)</span>
</code></pre></div><h3 id=tail-bdc-spark-logs>Tail BDC spark logs</h3><p>Logs on BDC are stored in a variety of places&mldr;</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash> tail -f /data/var/log/spark2-thrift/spark-spark-org.apache.spark.deploy.history.HistoryServer-1-aebigdata-bdcsce-1.out
</code></pre></div><p>reveals the connection AND the subsequent error!</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>18/05/16 16:18:53 DEBUG Client: IPC Client <span style=color:#f92672>(</span>2118255842<span style=color:#f92672>)</span> connection to /100.65.22.122:8010 from blk_1073742054_1231 got value <span style=color:#75715e>#1156</span>
18/05/16 16:18:53 DEBUG ProtobufRpcEngine: Call: getReplicaVisibleLength took 10ms
18/05/16 16:18:53 DEBUG Client: stopping client from cache: org.apache.hadoop.ipc.Client@6399551e
18/05/16 16:18:53 ERROR FsHistoryProvider: Exception encountered when attempting to load application log hdfs://aebigdata-bdcsce-1.compute-599616642.oraclecloud.internal:8020/spark-history/application_1523503765653_0002.inprogress
java.io.IOException: Cannot obtain block length <span style=color:#66d9ef>for</span> LocatedBlock<span style=color:#f92672>{</span>BP-2031294994-100.65.22.122-1523503732419:blk_1073742054_1231; getBlockSize<span style=color:#f92672>()=</span>244; corrupt<span style=color:#f92672>=</span>false; offset<span style=color:#f92672>=</span>0; locs<span style=color:#f92672>=[</span>DatanodeInfoWithStorage<span style=color:#f92672>[</span>100.65.22.122:50010,DS-f8d560be-7098-4529-9b01-7e26b089b17e,DISK<span style=color:#f92672>]]}</span>
        at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength<span style=color:#f92672>(</span>DFSInputStream.java:428<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength<span style=color:#f92672>(</span>DFSInputStream.java:336<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSInputStream.openInfo<span style=color:#f92672>(</span>DFSInputStream.java:272<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSInputStream.&lt;init&gt;<span style=color:#f92672>(</span>DFSInputStream.java:264<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DFSClient.open<span style=color:#f92672>(</span>DFSClient.java:1540<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall<span style=color:#f92672>(</span>DistributedFileSystem.java:304<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DistributedFileSystem$3.doCall<span style=color:#f92672>(</span>DistributedFileSystem.java:300<span style=color:#f92672>)</span>
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve<span style=color:#f92672>(</span>FileSystemLinkResolver.java:81<span style=color:#f92672>)</span>
        at org.apache.hadoop.hdfs.DistributedFileSystem.open<span style=color:#f92672>(</span>DistributedFileSystem.java:300<span style=color:#f92672>)</span>
        at org.apache.hadoop.fs.FileSystem.open<span style=color:#f92672>(</span>FileSystem.java:767<span style=color:#f92672>)</span>
        at org.apache.spark.scheduler.EventLoggingListener$.openEventLog<span style=color:#f92672>(</span>EventLoggingListener.scala:301<span style=color:#f92672>)</span>
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$replay<span style=color:#f92672>(</span>FsHistoryProvider.scala:643<span style=color:#f92672>)</span>
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$mergeApplicationListing<span style=color:#f92672>(</span>FsHistoryProvider.scala:460<span style=color:#f92672>)</span>
        at org.apache.spark.deploy.history.FsHistoryProvider$$anonfun$checkForLogs$3$$anon$4.run<span style=color:#f92672>(</span>FsHistoryProvider.scala:352<span style=color:#f92672>)</span>
        at java.util.concurrent.Executors$RunnableAdapter.call<span style=color:#f92672>(</span>Executors.java:511<span style=color:#f92672>)</span>
        at java.util.concurrent.FutureTask.run<span style=color:#f92672>(</span>FutureTask.java:266<span style=color:#f92672>)</span>
        at java.util.concurrent.ThreadPoolExecutor.runWorker<span style=color:#f92672>(</span>ThreadPoolExecutor.java:1149<span style=color:#f92672>)</span>
        at java.util.concurrent.ThreadPoolExecutor$Worker.run<span style=color:#f92672>(</span>ThreadPoolExecutor.java:624<span style=color:#f92672>)</span>
        at java.lang.Thread.run<span style=color:#f92672>(</span>Thread.java:748<span style=color:#f92672>)</span>
18/05/16 16:18:53 DEBUG Client: IPC Client <span style=color:#f92672>(</span>2118255842<span style=color:#f92672>)</span> connection to /100.65.22.122:8010 from blk_1073742054_1231: closed
18/05/16 16:18:53 DEBUG Client: IPC Client <span style=color:#f92672>(</span>2118255842<span style=color:#f92672>)</span> connection to /100.65.22.122:8010 from blk_1073742054_1231: stopped, remaining connections <span style=color:#ae81ff>1</span>
</code></pre></div><h3 id=check-on-last-log>Check on last log</h3><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>vim /data/var/log/spark2-thrift/spark-hive-org.apache.spark.sql.hive.thriftserver.HiveThriftServer2-1-aebigdata-bdcsce-1.out
</code></pre></div><p>more of the GC errors! Okay, corroborating evidence points to the JVM. We do have a lot of data hehehe&mldr;.</p><pre><code>18/05/16 19:01:40 ERROR SparkExecuteStatementOperation: Error executing query, currentState RUNNING,
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.lang.StringCoding.decode(StringCoding.java:215)
        at java.lang.String.&lt;init&gt;(String.java:463)
        at java.lang.String.&lt;init&gt;(String.java:515)
        at org.apache.spark.unsafe.types.UTF8String.toString(UTF8String.java:1005)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply_6$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.fromRow(ExpressionEncoder.scala:303)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2378)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1$$anonfun$apply$13.apply(Dataset.scala:2378)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2378)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
        at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2780)
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2377)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2382)
        at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2382)
        at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2793)
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2382)
        at org.apache.spark.sql.Dataset.collect(Dataset.scala:2358)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:245)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
18/05/16 19:01:40 ERROR SparkExecuteStatementOperation: Error running hive query:
org.apache.hive.service.cli.HiveSQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:266)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:174)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1$$anon$2.run(SparkExecuteStatementOperation.scala:171)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$1.run(SparkExecuteStatementOperation.scala:184)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
</code></pre><h3 id=thank-you-stackoverflow>Thank You StackOverflow</h3><p>So I asked the interwebs and they replied &ldquo;over yonder website doth thy answer live&rdquo; and pointed me to the below link</p><p><a href=https://stackoverflow.com/questions/46979848/spark-thriftserver-stops-or-freezes-due-to-tableau-queries>https://stackoverflow.com/questions/46979848/spark-thriftserver-stops-or-freezes-due-to-tableau-queries</a></p><p>I added <code>spark.sql.thriftServer.incrementalCollect=true</code> to <code>spark2-thirft-sparkconf.xml</code> in Ambari.
I went ahead and beefed up our <code>spark-env.sh</code> too</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>SPARK_EXECUTOR_CORES<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2&#34;</span> <span style=color:#75715e>#Number of cores for the workers (Default: 1).</span>
SPARK_EXECUTOR_MEMORY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;2G&#34;</span> <span style=color:#75715e>#Memory per Worker (e.g. 1000M, 2G) (Default: 1G)</span>
SPARK_DRIVER_MEMORY<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1080M&#34;</span> <span style=color:#75715e>#Memory for Master (e.g. 1000M, 2G) (Default: 512 Mb)</span>
</code></pre></div></div></main><footer><p class=text-muted>Powered by <a href=https://gohugo.io>Hugo</a> & <a href=https://github.com/calintat/minimal>Minimal</a> & <a href=https://github.com>GitHub</a> & Green Tea ❤️</p></footer></body></html>